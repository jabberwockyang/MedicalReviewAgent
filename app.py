import argparse
import json
import time
import os
import glob
import random
import shutil
from enum import Enum
from threading import Thread
from multiprocessing import Process, Value

import gradio as gr
import pytoml
from loguru import logger
import spaces

from huixiangdou.service import Worker, llm_serve, ArticleRetrieval, CacheRetriever, FeatureStore, FileOperation

class PARAM_CODE(Enum):
    """Parameter code."""
    SUCCESS = 0
    FAILED = 1
    ERROR = 2

def parse_args():
    """Parse args."""
    parser = argparse.ArgumentParser(description='Worker.')
    parser.add_argument('--work_dir',
                        type=str,
                        default='workdir',
                        help='Working directory.')
    parser.add_argument('--repo_dir',
                        type=str,
                        default='repodir',
                        help='Repository directory.')
    parser.add_argument(
        '--config_path',
        default='config.ini',
        type=str,
        help='Worker configuration path. Default value is config.ini')
    parser.add_argument('--standalone',
                        action='store_true',
                        default=True,
                        help='Auto deploy required Hybrid LLM Service.')
    parser.add_argument("--model_downloaded",
                        type=bool,
                        default=False,
                        help="If the model has been downloaded in the root/models folder. Default is False.")
    args = parser.parse_args()
    return args

def update_remote_buttons(remote):
    if remote:
        return [
                gr.Markdown("[å¦‚ä½•é…ç½®API]('https://github.com/jabberwockyang/MedicalReviewAgent/blob/main/README.md')",
                            visible=True),
                gr.Dropdown(["kimi", "deepseek", "zhipuai",'gpt'],
                                                label="é€‰æ‹©å¤§æ¨¡å‹æä¾›å•†",
                                                interactive=True,visible=True),
                gr.Textbox(label="æ‚¨çš„API",lines = 1,
                        interactive=True,visible=True),
                gr.Textbox(label="base url",lines = 1,
                        interactive=True,visible=True),
                gr.Dropdown([],label="é€‰æ‹©æ¨¡å‹",
                            interactive=True,visible=True)
        ]
    else:
        return [
                gr.Markdown("[å¦‚ä½•é…ç½®API]('https://github.com/jabberwockyang/MedicalReviewAgent/blob/main/README.md')",
                            visible=False),
                gr.Dropdown(["kimi", "deepseek", "zhipuai",'gpt'],
                                                label="é€‰æ‹©å¤§æ¨¡å‹æä¾›å•†",
                                                interactive=False,visible=False),
                gr.Textbox(label="æ‚¨çš„API",lines = 1,
                        interactive=False,visible=False),
                gr.Dropdown([],label="é€‰æ‹©æ¨¡å‹",
                            interactive=False,visible=False)
        ]

def udate_model_dropdown(remote_company):
    model_choices = {
        'kimi': ['moonshot-v1-128k'],
        'deepseek': ['deepseek-chat'],
        'zhipuai': ['glm-4'],
        'gpt': ['gpt-4-32k-0613','gpt-3.5-turbo']
    }
    return gr.Dropdown(choices= model_choices[remote_company])

def update_remote_config(remote_ornot,remote_company = None,api = None,baseurl = None, model = None):
    with open(CONFIG_PATH, encoding='utf8') as f:
        config = pytoml.load(f)
         
        if remote_ornot:
            if remote_company == None or api == None or model == None:
                raise ValueError('remote_company, api, model not provided')
            config['llm']['enable_local'] = 0
            config['llm']['enable_remote'] = 1
            config['llm']['server']['remote_type'] = remote_company
            config['llm']['server']['remote_api_key'] = api
            config['llm']['server']['remote_base_url'] = baseurl
            config['llm']['server']['remote_llm_model'] = model
        else:
            config['llm']['enable_local'] = 1
            config['llm']['enable_remote'] = 0
    with open(CONFIG_PATH, 'w') as f:
        pytoml.dump(config, f)
    return gr.Button("é…ç½®å·²ä¿å­˜")

# @spaces.GPU(duration=120)
def get_ready(query:str,chunksize=None,k=None,use_abstract=False):
    
    with open(CONFIG_PATH, encoding='utf8') as f:
        config = pytoml.load(f)
    workdir = config['feature_store']['work_dir']
    repodir = config['feature_store']['repo_dir']

    if query == 'repo_work': # no need to return assistant
        return repodir, workdir, config
    theme = ''
    try:
        with open(os.path.join(config['feature_store']['repo_dir'],'info.json'), 'r') as f:
            repo_info = json.load(f)
        theme = ' '.join(repo_info['keywords'])
    except:
        pass


    if use_abstract:
        workdir = workdir + '_ab'
    if query == 'annotation':
        if not chunksize or not k:
            raise ValueError('chunksize or k not provided')
        chunkdir = os.path.join(workdir, f'chunksize_{chunksize}')
        clusterdir = os.path.join(chunkdir, 'cluster_features', f'cluster_features_{k}')
        assistant = Worker(work_dir=chunkdir, config_path=CONFIG_PATH,language='en')
        samples_json = os.path.join(clusterdir,'samples.json')
        with open(samples_json, 'r') as f:
            samples = json.load(f)
            f.close()
        return clusterdir, samples, assistant, theme
    
    elif query == 'inspiration':
        if not chunksize or not k:
            raise ValueError('chunksize or k not provided')
        
        chunkdir = os.path.join(workdir, f'chunksize_{chunksize}')
        clusterdir = os.path.join(chunkdir, 'cluster_features', f'cluster_features_{k}')
        assistant = Worker(work_dir=chunkdir, config_path=CONFIG_PATH,language='en')
        annofile = os.path.join(clusterdir,'annotation.jsonl')
        with open(annofile, 'r') as f:
            annoresult = f.readlines()

            f.close()
        annoresult = [json.loads(obj) for obj in annoresult]
        return clusterdir, annoresult, assistant, theme
    elif query == 'summarize': # no need for params k
        if not chunksize:
            raise ValueError('chunksize not provided')
        chunkdir = os.path.join(workdir, f'chunksize_{chunksize}')
        assistant = Worker(work_dir=chunkdir, config_path=CONFIG_PATH,language='en')
        return assistant,theme

    else:
        raise ValueError('query not recognized')
            
def update_repo_info():
    with open(CONFIG_PATH, encoding='utf8') as f:
        config = pytoml.load(f)
    repodir = config['feature_store']['repo_dir']
    if os.path.exists(repodir):
        pdffiles = glob.glob(os.path.join(repodir, '*.pdf'))
        number_of_pdf = len(pdffiles)
        # åˆ¤æ–­info.jsonæ˜¯å¦å­˜åœ¨
        if os.path.exists(os.path.join(repodir,'info.json')):
                
            with open(os.path.join(repodir,'info.json'), 'r') as f:
                repo_info = json.load(f)

            keywords = repo_info['keywords']
            retmax = repo_info['retmax']
            search_len = len(repo_info['search_pmids'])
            import_len = len(repo_info['import_pmids'])
            failed_pmid_len = len(repo_info['failed_pmids'])

            pmc_success = repo_info['pmc_success_d']
            scihub_success = repo_info['scihub_success_d']
            failed_download = repo_info['failed_download']
            abstract_success = repo_info['abstract_success']
            failed_abstract = repo_info['failed_abstract']

            number_of_upload = number_of_pdf-scihub_success
            return keywords, retmax, search_len, import_len, failed_pmid_len, pmc_success, scihub_success, number_of_pdf, failed_download, number_of_upload, abstract_success, failed_abstract, number_of_pdf
        else:
            return None,None,None,None,None,None,None,None,None,number_of_pdf
    else:
        return None,None,None,None,None,None,None,None,None,None
               
def upload_file(files):
    repodir, workdir, _ = get_ready('repo_work')
    if not os.path.exists(repodir):
        os.makedirs(repodir)

    for file in files:
        destination_path = os.path.join(repodir, os.path.basename(file.name))

        shutil.copy(file.name, destination_path)
    

    return files

def generate_articles_repo(keys:str,pmids,retmax:int):
    
    keys = [k.strip() for k in keys.split('\n')]
    pmids = [k.strip() for k in pmids.split('\n')]
    pmids = [k for k in pmids if k.isdigit()]
    
    repodir, _, _ = get_ready('repo_work')

    articelfinder = ArticleRetrieval(keywords = keys,
                                     pmids = pmids,
                                     repo_dir = repodir,
                                     retmax = retmax)
    articelfinder.initiallize()
    return update_repo()

def delete_articles_repo():
    # åœ¨è¿™é‡Œè¿è¡Œç”Ÿæˆæ•°æ®åº“çš„å‡½æ•°
    repodir, workdir, _ = get_ready('repo_work')
    if os.path.exists(repodir):
        shutil.rmtree(repodir)
        shutil.rmtree(repodir + '_ab')

    if os.path.exists(workdir):
        shutil.rmtree(workdir)
        shutil.rmtree(workdir + '_ab')

    return gr.Textbox(label="æ–‡çŒ®åº“æ¦‚å†µ",lines =3,
                      value = 'æ–‡çŒ®åº“å’Œç›¸å…³æ•°æ®åº“å·²åˆ é™¤',
                      visible = True)

def update_repo():
    keys, retmax, search_len, import_len, _, pmc_success, scihub_success, pdflen, failed, abstract_success, failed_abstract, pdflen = update_repo_info()
    newinfo = ""
    if keys == None:
        newinfo += 'æ— å…³é”®è¯æœç´¢ç›¸å…³ä¿¡æ¯\n'
        newinfo += 'æ— å¯¼å…¥çš„PMID\n'
        if pdflen:
            newinfo += f'ä¸Šä¼ çš„PDFæ•°é‡: {pdflen}\n'
        else:
            newinfo += 'æ— ä¸Šä¼ çš„PDF\n'
    else:
        newinfo += f'å…³é”®è¯æœç´¢:'
        newinfo += f'   å…³é”®è¯: {keys}\n'
        newinfo += f'   æœç´¢ä¸Šé™: {retmax}\n'
        newinfo += f'   æœç´¢åˆ°çš„PMIDæ•°é‡: {search_len}\n'
        newinfo += f'å¯¼å…¥çš„PMIDæ•°é‡: {import_len}\n'
        newinfo += f'æˆåŠŸè·å–PMCå…¨æ–‡æ•°é‡: {pmc_success}\n'
        newinfo += f'æˆåŠŸè·å–SciHubå…¨æ–‡æ•°é‡: {scihub_success}\n'
        newinfo += f"ä¸‹è½½å¤±è´¥çš„ID: {failed}\n"
        newinfo += f"æˆåŠŸè·å–æ‘˜è¦çš„æ•°é‡: {abstract_success}\n"
        newinfo += f"è·å–æ‘˜è¦å¤±è´¥çš„æ•°é‡: {failed_abstract}\n"
        newinfo += f'ä¸Šä¼ çš„PDFæ•°é‡: {pdflen}\n'
   
    return gr.Textbox(label="æ–‡çŒ®åº“æ¦‚å†µ",lines =1,
                      value = newinfo,
                      visible = True)

def update_database_info():
    with open(CONFIG_PATH, encoding='utf8') as f:
        config = pytoml.load(f)
    workdir = config['feature_store']['work_dir']
    abworkdir = workdir + '_ab' 
    options = []
    total_json_obj = {}
    for dir in [workdir,abworkdir]:
        tag = 'Full Text' if '_ab' not in dir else 'Abstract'

        chunkdirs = glob.glob(os.path.join(dir, 'chunksize_*'))
        chunkdirs.sort()
        list_of_chunksize = [int(chunkdir.split('_')[-1]) for chunkdir in chunkdirs]
        # print(list_of_chunksize)
        jsonobj = {}
        for chunkdir in chunkdirs:
            k_dir = glob.glob(os.path.join(chunkdir, 'cluster_features','cluster_features_*'))
            k_dir.sort()
            list_of_k = [int(k.split('_')[-1]) for k in k_dir]
            jsonobj[int(chunkdir.split('_')[-1])] = list_of_k
        
        total_json_obj[dir] = jsonobj
        newoptions = [f"{tag}, chunksize:{chunksize}, k:{k}" for chunksize in list_of_chunksize for k in jsonobj[chunksize]]
        options.extend(newoptions)
    
    return options, total_json_obj

@spaces.GPU(duration=120)
def generate_database(chunksize:int,nclusters:str|list[str]):
    # åœ¨è¿™é‡Œè¿è¡Œç”Ÿæˆæ•°æ®åº“çš„å‡½æ•°
    repodir, workdir, _ = get_ready('repo_work')
    abrepodir = repodir + '_ab'
    abworkdir = workdir + '_ab'
    if not os.path.exists(repodir):
        return gr.Textbox(label="æ•°æ®åº“å·²ç”Ÿæˆ",value = 'è¯·å…ˆç”Ÿæˆæ–‡çŒ®åº“',visible = True)
    nclusters = [int(i) for i in nclusters]
    # æ–‡çŒ®åº“å’Œæ•°æ®åº“çš„è¦†ç›–åˆ é™¤é€»è¾‘å¾…å®š 
    # ç†è®ºä¸Š æ–‡çŒ®åº“åªèƒ½ç”Ÿæˆä¸€æ¬¡ æ‰€ä»¥æ¯æ¬¡ç”Ÿæˆæ–‡çŒ®åº“éƒ½è¦åˆ é™¤ä¹‹å‰çš„æ–‡çŒ®åº“å’Œæ•°æ®åº“
    # æ•°æ®åº“å¯ä»¥æ ¹æ®æ–‡çŒ®åº“å¤šæ¬¡ç”Ÿæˆ æš‚ä¸åšåˆ é™¤ ç›®å‰æ²¡æœ‰èŠ‚çœç®—åŠ›çš„é€»è¾‘ é‡å¤è®¡ç®—åè¦†ç›– ä»¥åä¼˜åŒ– 
    # ä¸åŒçš„chunksizeå’Œnclustersä¼šæ”¾åœ¨ä¸åŒçš„æ–‡ä»¶å¤¹ä¸‹ ä¸ä¼šäº’ç›¸è¦†ç›–
    # if os.path.exists(workdir):
    #     shutil.rmtree(workdir)

    cache = CacheRetriever(config_path=CONFIG_PATH)
    fs_init = FeatureStore(embeddings=cache.embeddings,
                           reranker=cache.reranker,
                            chunk_size=chunksize,
                            n_clusters=nclusters,
                           config_path=CONFIG_PATH)
    file_opr = FileOperation()

    # walk all files in repo dir
    files = file_opr.scan_dir(repo_dir=repodir)
    fs_init.initialize(files=files, work_dir=workdir,file_opr=file_opr)
    file_opr.summarize(files)

    files = file_opr.scan_dir(repo_dir=abrepodir)
    fs_init.initialize(files=files, work_dir=abworkdir,file_opr=file_opr)
    file_opr.summarize(files)

    del fs_init
    cache.pop('default')
    texts, _ = update_database_info()
    return gr.Textbox(label="æ•°æ®åº“æ¦‚å†µ",value = '\n'.join(texts) ,visible = True)

def delete_database():
    _, workdir, _ = get_ready('repo_work')
    if os.path.exists(workdir):
        shutil.rmtree(workdir)
        shutil.rmtree(workdir+'_ab')
    return  gr.Textbox(label="æ•°æ®åº“æ¦‚å†µ",lines =3,value = 'æ•°æ®åº“å·²åˆ é™¤',visible = True)

def update_database_textbox():
    texts, _ = update_database_info()
    if texts == []:
        return gr.Textbox(label="æ•°æ®åº“æ¦‚å†µ",value = 'ç›®å‰è¿˜æ²¡æœ‰æ•°æ®åº“',visible = True)
    else:
        return gr.Textbox(label="æ•°æ®åº“æ¦‚å†µ",value = '\n'.join(texts),visible = True)

def update_chunksize_dropdown():
    _, jsonobj = update_database_info()
    return gr.Dropdown(choices= jsonobj.keys())

def update_ncluster_dropdown(chunksize:int):
    _, jsonobj = update_database_info()
    nclusters = jsonobj[chunksize]
    return gr.Dropdown(choices= nclusters)

# @spaces.GPU(duration=120)
def annotation(n,chunksize:int,nclusters:int,remote_ornot:bool,use_abstract:bool):
    '''
    use llm to annotate cluster
    n: percentage of clusters to annotate
    '''
    query = 'annotation'
    if remote_ornot:
        backend = 'remote'
    else:
        backend = 'local'

    clusterdir, samples, assistant, theme = get_ready('annotation',chunksize,nclusters,use_abstract)
    new_obj_list = []
    n = round(n * len(samples.keys()))
    for cluster_no in random.sample(samples.keys(), n):
        chunk = '\n'.join(samples[cluster_no]['samples'][:10])

        code, reply, cluster_no = assistant.annotate_cluster(
                                                theme = theme,
                                                cluster_no=cluster_no,
                                                chunk=chunk,
                                                history=[],
                                                groupname='',
                                                backend=backend)
        references = f"cluster_no: {cluster_no}"
        new_obj = {
            'cluster_no': cluster_no,
            'chunk': chunk,
            'annotation': reply
        }
        new_obj_list.append(new_obj)
        logger.info(f'{code}, {query}, {reply}, {references}')

        with open(os.path.join(clusterdir, 'annotation.jsonl'), 'a') as f:
            json.dump(new_obj, f, ensure_ascii=False)
            f.write('\n')
            
    return '\n\n'.join([obj['annotation'] for obj in new_obj_list])

# @spaces.GPU(duration=120)
def inspiration(annotation:str,chunksize:int,nclusters:int,remote_ornot:bool,use_abstract:bool):
    query = 'inspiration'
    if remote_ornot:
        backend = 'remote'
    else:
        backend = 'local'
        
    clusterdir, annoresult, assistant, theme = get_ready('inspiration',chunksize,nclusters,use_abstract)
    new_obj_list = []

    if annotation is not None: # if the user wants to get inspiration from specific clusters only  
        annoresult = [obj for obj in annoresult if obj['annotation'] in [txt.strip() for txt in annotation.split('\n')]]
    
    for index in random.sample(range(len(annoresult)), min(5, len(annoresult))):
        cluster_no = annoresult[index]['cluster_no']
        chunks = annoresult[index]['annotation']
        
        code, reply = assistant.getinspiration(
                                                theme = theme,
                                                annotations = chunks,
                                                history=[], 
                                                groupname='',backend=backend)
        new_obj = {
            'inspiration': reply,
            'cluster_no': cluster_no
        }
        new_obj_list.append(new_obj)
        logger.info(f'{code}, {query}, {cluster_no},{reply}')

        with open(os.path.join(clusterdir, 'inspiration.jsonl'), 'a') as f:
            json.dump(new_obj, f, ensure_ascii=False)
        with open(os.path.join(clusterdir, 'inspiration.txt'), 'a') as f:
            f.write(f'{reply}\n')
            
    return '\n\n'.join(list(set([obj['inspiration'] for obj in new_obj_list])))


def getpmcurls(references):
    urls = []
    for ref in references:
        if ref.startswith('PMC'):
            
            refid = ref.replace('.txt','')
            urls.append(f'https://www.ncbi.nlm.nih.gov/pmc/articles/{refid}/')
        else:
            urls.append(ref)
    return urls
    
@spaces.GPU(duration=120)
def summarize_text(query,chunksize:int,remote_ornot:bool,use_abstract:bool):
    if remote_ornot:
        backend = 'remote'
    else:
        backend = 'local'
        
    assistant,_ = get_ready('summarize',chunksize=chunksize,k=None,use_abstract=use_abstract)
    code, reply, references = assistant.generate(query=query,
                                                history=[],
                                                groupname='',backend = backend)
      
    logger.info(f'{code}, {query}, {reply}, {references}')
    urls = getpmcurls(references)
    mds = '\n\n'.join([f'[{ref}]({url})' for ref,url in zip(references,urls)])
    return gr.Markdown(label="çœ‹çœ‹",value = reply,line_breaks=True) , gr.Markdown(label="å‚è€ƒæ–‡çŒ®",value = mds,line_breaks=True) 

def main_interface():   
    with gr.Blocks() as demo:
        with gr.Row():
            gr.Markdown(
                """
                # åŒ»å­¦æ–‡çŒ®ç»¼è¿°åŠ©æ‰‹ (åˆå ä¸æƒ³çœ‹æ–‡çŒ®)
                """
            )

        with gr.Tab("æ¨¡å‹æœåŠ¡é…ç½®"):
            gr.Markdown("""
            #### é…ç½®æ¨¡å‹æœåŠ¡ ğŸ› ï¸

            1. **æ˜¯å¦ä½¿ç”¨è¿œç¨‹å¤§æ¨¡å‹**
            - å‹¾é€‰æ­¤é¡¹ï¼Œå¦‚æœä½ æƒ³ä½¿ç”¨è¿œç¨‹çš„å¤§æ¨¡å‹æœåŠ¡ã€‚
            - å¦‚æœä¸å‹¾é€‰ï¼Œå°†é»˜è®¤ä½¿ç”¨æœ¬åœ°æ¨¡å‹æœåŠ¡ã€‚

            2. **APIé…ç½®**
            - é…ç½®å¤§æ¨¡å‹æä¾›å•†å’ŒAPIï¼Œç¡®ä¿æ¨¡å‹æœåŠ¡èƒ½å¤Ÿæ­£å¸¸è¿è¡Œã€‚
            - æä¾›å•†é€‰æ‹©ï¼škimiã€deepseekã€zhipuaiã€gptã€‚
            - è¾“å…¥æ‚¨çš„APIå¯†é’¥å’Œé€‰æ‹©å¯¹åº”æ¨¡å‹ã€‚
            - ç‚¹å‡»â€œä¿å­˜é…ç½®â€æŒ‰é’®ä»¥ä¿å­˜æ‚¨çš„è®¾ç½®ã€‚

            ğŸ“ **å¤‡æ³¨**ï¼šè¯·å‚è€ƒ[å¦‚ä½•ä½¿ç”¨]('https://github.com/jabberwockyang/MedicalReviewAgent/blob/main/README.md')è·å–æ›´å¤šä¿¡æ¯ã€‚

            """)

            remote_ornot = gr.Checkbox(label="æ˜¯å¦ä½¿ç”¨è¿œç¨‹å¤§æ¨¡å‹")
            with gr.Accordion("APIé…ç½®", open=True):
                apimd = gr.Markdown("[å¦‚ä½•é…ç½®API]('https://github.com/jabberwockyang/MedicalReviewAgent/blob/main/README.md')",visible=False)
                remote_company = gr.Dropdown(["kimi", "deepseek", "zhipuai",'gpt'],
                                            label="é€‰æ‹©å¤§æ¨¡å‹æä¾›å•†",interactive=False,visible=False)
                api = gr.Textbox(label="æ‚¨çš„API",lines = 1,interactive=False,visible=False)
                baseurl = gr.Textbox(label="base url",lines = 1,interactive=False,visible=False)
                model = gr.Dropdown([],label="é€‰æ‹©æ¨¡å‹",interactive=False,visible=False)
            
            confirm_button = gr.Button("ä¿å­˜é…ç½®")

            remote_ornot.change(update_remote_buttons, inputs=[remote_ornot],outputs=[apimd,remote_company,api,baseurl,model])
            remote_company.change(udate_model_dropdown, inputs=[remote_company],outputs=[model])
            confirm_button.click(update_remote_config, inputs=[remote_ornot,remote_company,api,baseurl,model],outputs=[confirm_button])


        with gr.Tab("æ–‡çŒ®æŸ¥æ‰¾+æ•°æ®åº“ç”Ÿæˆ"):
            gr.Markdown("""
#### æŸ¥æ‰¾æ–‡çŒ® ğŸ“š

1. **è¾“å…¥å…³é”®è¯æˆ–PMIDæ‰¹é‡PubMed PMCæ–‡çŒ®**
   - åœ¨â€œæ„Ÿå…´è¶£çš„å…³é”®è¯â€æ¡†ä¸­è¾“å…¥æ‚¨æ„Ÿå…´è¶£çš„å…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ªã€‚
   - è®¾ç½®æŸ¥æ‰¾æ•°é‡ï¼ˆ0-500ï¼‰ã€‚
   - åœ¨â€œè¾“å…¥PMIDâ€æ¡†ä¸­è¾“å…¥åœ¨PubMedä¸­å¯¼å‡ºçš„PMIDï¼Œæ¯è¡Œä¸€ä¸ªã€‚
   - ç‚¹å‡»â€œæœç´¢PubMed å¹¶æ‹‰å–å…¨æ–‡â€æŒ‰é’®è¿›è¡Œæ–‡çŒ®æŸ¥æ‰¾ã€‚ç›®å‰ä¸»è¦åŸºäºPMCæ•°æ®åº“å’Œscihub, åœ¨PMCä¸­æœªæ”¶å½•çš„æ–‡çŒ®å°†ä½¿ç”¨scihubä¸‹è½½ï¼Œscihubè¿‘å¹´æ–‡çŒ®æœªæ”¶å½•
                        
2. **ä¸Šä¼ PDF**
   - é€šè¿‡â€œä¸Šä¼ PDFâ€æŒ‰é’®ä¸Šä¼ æ‚¨å·²æœ‰çš„PDFæ–‡çŒ®æ–‡ä»¶ã€‚

3. **æ›´æ–°æ–‡çŒ®åº“æƒ…å†µ åˆ é™¤æ–‡çŒ®åº“**
   - ç‚¹å‡»â€œæ›´æ–°æ–‡çŒ®åº“æƒ…å†µâ€æŒ‰é’®ï¼ŒæŸ¥çœ‹å½“å‰æ–‡çŒ®åº“çš„æ¦‚å†µã€‚
   - å¦‚æœéœ€è¦é‡ç½®æˆ–åˆ é™¤ç°æœ‰æ–‡çŒ®åº“ï¼Œç‚¹å‡»â€œåˆ é™¤æ–‡çŒ®åº“â€æŒ‰é’®ã€‚


#### ç”Ÿæˆæ•°æ®åº“ ğŸ—‚ï¸

1. **è®¾ç½®æ•°æ®åº“æ„å»ºå‚æ•° ç”Ÿæˆæ•°æ®åº“**
   - é€‰æ‹©å—å¤§å°ï¼ˆChunk Sizeï¼‰å’Œèšç±»æ•°ï¼ˆNumber of Clustersï¼‰ã€‚
   - æä¾›é€‰é¡¹ç”¨äºé€‰æ‹©åˆé€‚çš„å—å¤§å°å’Œèšç±»æ•°ã€‚
   - ç‚¹å‡»â€œç”Ÿæˆæ•°æ®åº“â€æŒ‰é’®å¼€å§‹æ•°æ®åº“ç”Ÿæˆè¿‡ç¨‹ã€‚

2. **æ›´æ–°æ•°æ®åº“æƒ…å†µ åˆ é™¤æ•°æ®åº“**
   - ç‚¹å‡»â€œæ›´æ–°æ•°æ®åº“æƒ…å†µâ€æŒ‰é’®ï¼ŒæŸ¥çœ‹å½“å‰æ•°æ®åº“çš„æ¦‚å†µã€‚
   - ç‚¹å‡»â€œåˆ é™¤æ•°æ®åº“â€æŒ‰é’®ç§»é™¤ç°æœ‰æ•°æ®åº“ã€‚

ğŸ“ **å¤‡æ³¨**ï¼šè¯·å‚è€ƒ[å¦‚ä½•é€‰æ‹©æ•°æ®åº“æ„å»ºå‚æ•°]('https://github.com/jabberwockyang/MedicalReviewAgent/tree/main')è·å–æ›´å¤šä¿¡æ¯ã€‚
""")
            with gr.Row(equal_height=True):
                with gr.Column(scale=1):
                    with gr.Row():
                        with gr.Column(scale=1):
                            input_keys = gr.Textbox(label="æ„Ÿå…´è¶£çš„å…³é”®è¯, æ¢è¡Œåˆ†éš”, ä¸å¤ªå¥½ç”¨åˆ«ç”¨ç­‰æˆ‘æ”¹æ”¹",
                                                    lines = 5)
                            retmax = gr.Slider(
                                    minimum=0,
                                    maximum=500,
                                    value=250,
                                    interactive=True,
                                    label="æœç´¢ä¸Šé™",
                                    info="How many articles you want to retrieve?"
                                )

                        with gr.Column(scale=1):
                            input_pmids = gr.Textbox(label="è¾“å…¥PMID, æ¢è¡Œåˆ†éš”",
                                                    lines = 5)
                    
                    generate_repo_button = gr.Button("æœç´¢PubMedå¹¶æ‹‰å–å…¨æ–‡")     

                with gr.Column(scale=1):
                    file_output = gr.File(scale=2)
                    upload_button = gr.UploadButton("ä¸Šä¼ PDF", 
                                    file_types=[".pdf"], 
                                    file_count="multiple",scale=1)
                    
            with gr.Row(equal_height=True):
                with gr.Column(scale=0):
                    delete_repo_button = gr.Button("åˆ é™¤æ–‡çŒ®åº“")
                    update_repo_button = gr.Button("æ›´æ–°æ–‡çŒ®åº“æƒ…å†µ")
                with gr.Column(scale=2):
                    repo_summary =gr.Textbox(label= 'æ–‡çŒ®åº“æ¦‚å†µ', 
                                             value="ç›®å‰è¿˜æ²¡æœ‰æ–‡çŒ®åº“")

            generate_repo_button.click(generate_articles_repo, 
                                inputs=[input_keys,input_pmids,retmax],
                                outputs = [repo_summary])
            
            delete_repo_button.click(delete_articles_repo, inputs=None,
                                outputs = repo_summary)
            update_repo_button.click(update_repo, inputs=None,
                                outputs = repo_summary)
            upload_button.upload(upload_file, upload_button, file_output)
            
            with gr.Accordion("æ•°æ®åº“æ„å»ºå‚æ•°", open=True):
                gr.Markdown("[å¦‚ä½•é€‰æ‹©æ•°æ®åº“æ„å»ºå‚æ•°]('https://github.com/jabberwockyang/MedicalReviewAgent/tree/main')")
                chunksize = gr.Slider(label="Chunk Size",
                                      info= 'How long you want the chunk to be?',
                                        minimum=128, maximum=4096,value=1024,step=1,
                                        interactive=True)
                ncluster = gr.CheckboxGroup(["10", "20", "50", '100','200','500','1000'], 
                                            label="Number of Clusters", 
                                            info="How many Clusters you want to generate")

            with gr.Row():
                gene_database_button = gr.Button("ç”Ÿæˆæ•°æ®åº“")
                delete_database_button = gr.Button("åˆ é™¤æ•°æ®åº“")
                update_database_button = gr.Button("æ›´æ–°æ•°æ®åº“æƒ…å†µ")

            database_summary = gr.Textbox(label="æ•°æ®åº“æ¦‚å†µ",lines = 1,value="ç›®å‰è¿˜æ²¡æœ‰æ•°æ®åº“")
            

            gene_database_button.click(generate_database, inputs=[chunksize,ncluster],
                                outputs = database_summary)
            
            update_database_button.click(update_database_textbox,inputs=None,
                                outputs = [database_summary])
                                         
            delete_database_button.click(delete_database, inputs=None,
                                outputs = database_summary)
        with gr.Tab("å†™ç»¼è¿°"):
            gr.Markdown("""
#### å†™ç»¼è¿° âœï¸

1. **æ›´æ–°æ•°æ®åº“æƒ…å†µ**
   - ç‚¹å‡»â€œæ›´æ–°æ•°æ®åº“æƒ…å†µâ€æŒ‰é’®ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„æ•°æ®åº“ä¿¡æ¯ã€‚

2. **é€‰æ‹©å—å¤§å°å’Œèšç±»æ•°**
   - ä»ä¸‹æ‹‰èœå•ä¸­é€‰æ‹©åˆé€‚çš„å—å¤§å°å’Œèšç±»æ•°ã€‚

3. **æŠ½æ ·æ ‡æ³¨æ–‡ç« èšç±»**
   - è®¾ç½®æŠ½æ ·æ ‡æ³¨æ¯”ä¾‹ï¼ˆ0-1ï¼‰ã€‚
   - ç‚¹å‡»â€œæŠ½æ ·æ ‡æ³¨æ–‡ç« èšç±»â€æŒ‰é’®å¼€å§‹æ ‡æ³¨è¿‡ç¨‹ã€‚

4. **è·å–çµæ„Ÿ**
   - å¦‚æœä¸çŸ¥é“å†™ä»€ä¹ˆï¼Œç‚¹å‡»â€œè·å–çµæ„Ÿâ€æŒ‰é’®ã€‚
   - ç³»ç»Ÿå°†åŸºäºæ ‡æ³¨çš„æ–‡ç« èšç±»æä¾›ç›¸åº”çš„ç»¼è¿°å­é—®é¢˜ã€‚

5. **å†™ç»¼è¿°**
   - è¾“å…¥æ‚¨æƒ³å†™çš„å†…å®¹æˆ–ä¸»é¢˜ã€‚
   - ç‚¹å‡»â€œå†™ç»¼è¿°â€æŒ‰é’®ï¼Œç”Ÿæˆç»¼è¿°æ–‡æœ¬ã€‚

6. **æŸ¥çœ‹ç”Ÿæˆç»“æœ**
   - ç”Ÿæˆçš„ç»¼è¿°æ–‡æœ¬å°†æ˜¾ç¤ºåœ¨â€œçœ‹çœ‹â€æ–‡æœ¬æ¡†ä¸­ã€‚
   - å‚è€ƒæ–‡çŒ®å°†æ˜¾ç¤ºåœ¨â€œå‚è€ƒæ–‡çŒ®â€æ¡†ä¸­ã€‚

ğŸ“ **å¤‡æ³¨**ï¼šå¯ä»¥å°è¯•ä¸åŒçš„å‚æ•°è¿›è¡Œæ ‡æ³¨å’Œçµæ„Ÿè·å–ï¼Œæœ‰åŠ©äºæé«˜ç»¼è¿°çš„è´¨é‡å’Œç›¸å…³æ€§ã€‚
            """)

            with gr.Accordion("èšç±»æ ‡æ³¨ç›¸å…³å‚æ•°", open=True):
                with gr.Row():
                    update_options = gr.Button("æ›´æ–°æ•°æ®åº“æƒ…å†µ", scale=0)
                    use_abstract = gr.Checkbox(label="æ˜¯å¦ä»…ä½¿ç”¨æ‘˜è¦",scale=0)
                    chunksize = gr.Dropdown([], label="é€‰æ‹©å—å¤§å°", scale=0)
                    nclusters = gr.Dropdown([], label="é€‰æ‹©èšç±»æ•°", scale=0)
                    ntoread = gr.Slider(
                            minimum=0,maximum=1,value=0.5,
                            interactive=True,
                            label="æŠ½æ ·æ ‡æ³¨æ¯”ä¾‹",
                        )

            annotation_button = gr.Button("æŠ½æ ·æ ‡æ³¨æ–‡ç« èšç±»")
            annotation_output =  gr.Textbox(label="æ–‡ç« èšç±»æ ‡æ³¨/ç‰‡æ®µæ‘˜è¦",
                                            lines = 5, 
                                            interactive= True,
                                            show_copy_button=True)
            inspiration_button = gr.Button("è·å–çµæ„Ÿ")
            inspiration_output = gr.Textbox(label="çµå…‰ä¸€ç°",
                                            lines = 5,
                                            show_copy_button=True)


            query = gr.Textbox(label="æƒ³å†™ä»€ä¹ˆ")
            
            write_button = gr.Button("å†™ç»¼è¿°")
            output_text = gr.Markdown(label="çœ‹çœ‹")
            output_references = gr.Markdown(label="å‚è€ƒæ–‡çŒ®")
            
            update_options.click(update_chunksize_dropdown,
                                outputs=[chunksize])
            
            chunksize.change(update_ncluster_dropdown, 
                             inputs=[chunksize],
                             outputs= [nclusters])
            
            annotation_button.click(annotation, 
                                    inputs = [ntoread, chunksize, nclusters,remote_ornot,use_abstract],
                                    outputs=[annotation_output])
            
            inspiration_button.click(inspiration, 
                                     inputs= [annotation_output, chunksize, nclusters,remote_ornot,use_abstract],
                                     outputs=[inspiration_output])
            
            write_button.click(summarize_text,
                                inputs=[query, chunksize,remote_ornot,use_abstract],
                                outputs =[output_text,output_references])

    demo.launch(share=False, server_name='0.0.0.0', debug=True,show_error=True,allowed_paths=['img_0.jpg']) 
 
# start service
if __name__ == '__main__':
    args = parse_args()
    # copy config from config-bak 
    if args.model_downloaded:
        shutil.copy('config-mod_down-bak.ini', args.config_path) # yyj
    else:
        shutil.copy('config-bak.ini', args.config_path) # yyj

    CONFIG_PATH = args.config_path

    if args.standalone is True:
        # hybrid llm serve
        server_ready = Value('i', 0)
        server_process = Process(target=llm_serve,
                                args=(args.config_path, server_ready))
        server_process.start()
        while True:
            if server_ready.value == 0:
                logger.info('waiting for server to be ready..')
                time.sleep(3)
            elif server_ready.value == 1:
                break
            else:
                logger.error('start local LLM server failed, quit.')
                raise Exception('local LLM path')
        logger.info('Hybrid LLM Server start.')

    main_interface()